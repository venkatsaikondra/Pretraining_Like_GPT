
What is Language Modeling?

Language modeling is the process of a machine learning model learning to predict the next word in a sequence of text, based on the words that came before it.

![img.png](images/img.png)

We, humans, already have some feeling of "probability" when it comes to natural language. For example, when we talk, usually we understand each other quite well (at least, what's being said). We disambiguate between different options which sound similar without even realizing it!

But how a machine is supposed to understand this? A machine needs a language model, which estimates the probabilities of sentences. If a language model is good, it will assign a larger probability to a correct option.

Examples of language models include GPT-4, BERT, and Llama 3.

GPT{Generative Pretrained Transformer}

the core stages that a model like GPT (Generative Pre-trained Transformer) goes through are

ğŸ“˜ RAW TEXT DATA
      â”‚
      â–¼
ğŸ§¹ 1. DATA PREPROCESSING
   - Collect large text corpora (books, web, etc.)
   - Clean, normalize, remove duplicates
      â”‚
      â–¼
ğŸ”¡ 2. TOKENIZATION
   - Split text into subword tokens (e.g., BPE)
   - Convert words â†’ numeric IDs
      â”‚
      â–¼
ğŸ§­ 3. EMBEDDING LAYER
   - Token embeddings â†’ represent meaning
   - Positional embeddings â†’ represent order
      â”‚
      â–¼
ğŸ” 4. TRANSFORMER BLOCKS (repeated N times)
   â”œâ”€â”€ Self-Attention (context across tokens)
   â”œâ”€â”€ Feed-Forward Network (nonlinear mapping)
   â”œâ”€â”€ Residual Connections + Layer Norm
      â”‚
      â–¼
ğŸ¯ 5. OUTPUT HEAD
   - Linear projection to vocabulary size
   - Softmax â†’ next-token probability
      â”‚
      â–¼
ğŸ“‰ 6. TRAINING OBJECTIVE
   - Causal Language Modeling (predict next token)
   - Loss = Cross-Entropy between predicted vs actual token
      â”‚
      â–¼
âš™ï¸ 7. OPTIMIZATION
   - Optimizer: AdamW
   - Learning rate schedule + gradient clipping
      â”‚
      â–¼
ğŸ’ª 8. PRETRAINED MODEL
   - Learns general language patterns
      â”‚
      â–¼
ğŸ“ 9. FINE-TUNING (optional)
   - Train on specific tasks (e.g., book verdicts)
   - Supervised fine-tuning or RLHF
      â”‚
      â–¼
ğŸ’¬ 10. INFERENCE
   - Input prompt â†’ model predicts next tokens
   - Generates text autoregressively

COLLECTING DATA

Now I am taking the data as the book "The Verdict"
![img_1.png](images/img_1.png)

Taking data from kaggle

---------------------------------------------------------------------------------------------------------

TOKENIZATION

Tokenization is the process of replacing sensitive data with a non-sensitive, unique equivalent called a "token"

Good tokenization web app :https://tiktokenizer.vercel.app

Byte pair encoder

Byte Pair Encoding (BPE) is a data compression and tokenization technique that repeatedly merges the most frequent pair of adjacent symbols (bytes or characters) into a single, new symbol

![img_2.png](images/img_2.png)

______________________________________________________________________________________________________________________________________________________________________________________________________

EMBEDDING LAYER

"""
Word Embeddings

(ã€œï¿£â–½ï¿£)ã€œ

Word Embeddings is â€œa popular and powerful way
to associate a vector with a wordâ€ ğŸš€

An excellent benefit to using Word Embeddings is that
it can â€œpack more information into far FEWER dimensionsâ€ than
the â€œword vectors obtained via One-Hot encodingâ€

I like the following remark about Word Embeddings ğŸ˜‰
from https://medium.com/aiguys/word-embeddings-cbow-and-skip-gram-5d615ad61d3d
â€œWord embeddings can be thought of as an alternate to one-hot encoding
along with dimensionality reduction.â€









